{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "import ndjson\n",
    "import pandas\n",
    "import cPickle as pkl\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = pandas.read_csv('../../data_all/bee.csv')\n",
    "strokes = json.loads(_tmp.iloc[20000,1])\n",
    "\n",
    "image = Image.new(\"P\", (256,256), color=255)\n",
    "image_draw = ImageDraw.Draw(image)\n",
    "for stroke in strokes:\n",
    "    for i in range(len(stroke[0])-1):\n",
    "        image_draw.line([stroke[0][i], \n",
    "                         stroke[1][i],\n",
    "                         stroke[0][i+1], \n",
    "                         stroke[1][i+1]],\n",
    "                        fill=0, width=5)\n",
    "image = image.resize((128,128))\n",
    "\n",
    "image.save('x.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[148, 133, 121, 42, 29, 21, 21, 26, 36, 52, 81, 182, 203, 210, 214, 217, 209, 183, 164, 137], [74, 71, 73, 105, 117, 132, 162, 176, 190, 202, 215, 217, 204, 190, 169, 101, 94, 82, 77, 75]]\n"
     ]
    }
   ],
   "source": [
    "print strokes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_name(p):\n",
    "    return p.split('/')[-1].split('.')[0]\n",
    "\n",
    "data_root = '../../data_all/'\n",
    "data_files = [os.path.join(data_root, x) for x in os.listdir(data_root)]\n",
    "class_id_map = {}\n",
    "for i, x in enumerate(data_files):\n",
    "    class_id_map[class_name(x)] = i\n",
    "\n",
    "def strokes_to_image_str(strokes):\n",
    "    image = Image.new(\"P\", (256,256), color=255)\n",
    "    image_draw = ImageDraw.Draw(image)\n",
    "    for stroke in strokes:\n",
    "        for i in range(len(stroke[0])-1):\n",
    "            image_draw.line([stroke[0][i], \n",
    "                             stroke[1][i],\n",
    "                             stroke[0][i+1], \n",
    "                             stroke[1][i+1]],\n",
    "                             fill=0, width=5)\n",
    "    img_size = 128\n",
    "    image = image.resize((img_size,img_size))\n",
    "\n",
    "    with io.BytesIO() as output:\n",
    "        image.save(output, format=\"PNG\")\n",
    "        contents = output.getvalue()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pkl_data = {}\n",
    "for d in data_files:\n",
    "    cn = class_name(d)\n",
    "    data = pandas.read_csv(d)\n",
    "    pkl_data[cn] = data\n",
    "\n",
    "pkl.dump(pkl_data, file('pkled_all.pkl', 'wb'), 2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pkl.load(file('pkled_all.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "tfr_root = '../../tfrecords_x4/'\n",
    "\n",
    "tot_num_train = 0\n",
    "tot_num_val = 0\n",
    "\n",
    "all_samples = []\n",
    "\n",
    "for cn in tqdm(data_all.keys()):\n",
    "    df = data_all[cn]\n",
    "    n_samples = len(df)\n",
    "    \n",
    "    cid = class_id_map[cn]\n",
    "    \n",
    "    tfr_val = os.path.join(tfr_root, 'val-%04d.tfrecord' % cid)\n",
    "    \n",
    "    n_samples_val = 1000\n",
    "    n_samples_train = 40000\n",
    "    \n",
    "    #n_samples_train = n_samples - 1000\n",
    "    \n",
    "    tot_num_train += n_samples_train\n",
    "    tot_num_val += n_samples_val\n",
    "    \n",
    "    \n",
    "    for i in range(n_samples_train):\n",
    "        img_raw = strokes_to_image_str(json.loads(df.iloc[i,1]))\n",
    "    \n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'label': _int64_feature(cid),\n",
    "            'image_raw': _bytes_feature(img_raw)}))\n",
    "        \n",
    "        all_samples.append(example.SerializeToString())\n",
    "    \n",
    "    \n",
    "    writer_val = tf.python_io.TFRecordWriter(tfr_val)\n",
    "    for i in range(n_samples - n_samples_val, n_samples):\n",
    "        img_raw = strokes_to_image_str(json.loads(df.iloc[i,1]))\n",
    "    \n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'label': _int64_feature(cid),\n",
    "            'image_raw': _bytes_feature(img_raw)}))\n",
    "        writer_val.write(example.SerializeToString())\n",
    "        \n",
    "    writer_val.close()\n",
    "    \n",
    "    \n",
    "sample_per_shard = len(all_samples) / 100\n",
    "if len(all_samples) % 100 != 0:\n",
    "    sample_per_shard += 1\n",
    "\n",
    "import random\n",
    "random.shuffle(all_samples)\n",
    "    \n",
    "for i in range(100):\n",
    "    tfr_train = os.path.join(tfr_root, 'train-%04d.tfrecord' % i)\n",
    "    writer_train = tf.python_io.TFRecordWriter(tfr_train)\n",
    "    \n",
    "    for j in range(sample_per_shard*i, min(sample_per_shard*(i+1), len(all_samples))):\n",
    "        writer_train.write(all_samples[j])\n",
    "    writer_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400000 340000 340\n"
     ]
    }
   ],
   "source": [
    "print tot_num_train, tot_num_val, len(class_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(class_id_map, file('class_id_map.pkl', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
